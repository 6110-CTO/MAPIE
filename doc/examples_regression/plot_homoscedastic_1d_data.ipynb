{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Estimate the prediction intervals of 1D homoscedastic data\n\n:class:`mapie.regression.MapieRegressor` is used to estimate\nthe prediction intervals of 1D homoscedastic data using\ndifferent strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\nfrom typing_extensions import TypedDict\n\nimport numpy as np\nimport scipy\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import pyplot as plt\n\nfrom mapie.regression import MapieRegressor\n\n\ndef f(x: np.ndarray) -> np.ndarray:\n    \"\"\"Polynomial function used to generate one-dimensional data\"\"\"\n    return np.array(5*x + 5*x**4 - 9*x**2)\n\n\ndef get_homoscedastic_data(\n    n_train: int = 200,\n    n_true: int = 200,\n    sigma: float = 0.1\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, float]:\n    \"\"\"\n    Generate one-dimensional data from a given function,\n    number of training and test samples and a given standard\n    deviation for the noise.\n    The training data data is generated from an exponential distribution.\n\n    Parameters\n    ----------\n    n_train : int, optional\n        Number of training samples, by default  200.\n    n_true : int, optional\n        Number of test samples, by default 1000.\n    sigma : float, optional\n        Standard deviation of noise, by default 0.1\n\n    Returns\n    -------\n    Tuple[Any, Any, np.ndarray, Any, float]\n        Generated training and test data.\n        [0]: X_train\n        [1]: y_train\n        [2]: X_true\n        [3]: y_true\n        [4]: y_true_sigma\n    \"\"\"\n    np.random.seed(59)\n    q95 = scipy.stats.norm.ppf(0.95)\n    X_train = np.linspace(0, 1, n_train)\n    X_true = np.linspace(0, 1, n_true)\n    y_train = f(X_train) + np.random.normal(0, sigma, n_train)\n    y_true = f(X_true)\n    y_true_sigma = q95*sigma\n    return X_train, y_train, X_true, y_true, y_true_sigma\n\n\ndef plot_1d_data(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    y_test: np.ndarray,\n    y_test_sigma: float,\n    y_pred: np.ndarray,\n    y_pred_low: np.ndarray,\n    y_pred_up: np.ndarray,\n    ax: plt.Axes,\n    title: str\n) -> None:\n    \"\"\"\n    Generate a figure showing the training data and estimated\n    prediction intervals on test data.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training data.\n    y_train : np.ndarray\n        Training labels.\n    X_test : np.ndarray\n        Test data.\n    y_test : np.ndarray\n        True function values on test data.\n    y_test_sigma : float\n        True standard deviation.\n    y_pred : np.ndarray\n        Predictions on test data.\n    y_pred_low : np.ndarray\n        Predicted lower bounds on test data.\n    y_pred_up : np.ndarray\n        Predicted upper bounds on test data.\n    ax : plt.Axes\n        Axis to plot.\n    title : str\n        Title of the figure.\n    \"\"\"\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    ax.scatter(X_train, y_train, color=\"red\", alpha=0.3, label=\"training\")\n    ax.plot(X_test, y_test, color=\"gray\", label=\"True confidence intervals\")\n    ax.plot(X_test, y_test - y_test_sigma, color=\"gray\", ls=\"--\")\n    ax.plot(X_test, y_test + y_test_sigma, color=\"gray\", ls=\"--\")\n    ax.plot(X_test, y_pred, label=\"Prediction intervals\")\n    ax.fill_between(X_test, y_pred_low, y_pred_up, alpha=0.3)\n    ax.set_title(title)\n    ax.legend()\n\n\nX_train, y_train, X_test, y_test, y_test_sigma = get_homoscedastic_data()\n\npolyn_model = Pipeline([\n    (\"poly\", PolynomialFeatures(degree=4)),\n    (\"linear\", LinearRegression(fit_intercept=False))\n])\n\nParams = TypedDict(\"Params\", {\"method\": str, \"cv\": int})\nSTRATEGIES = {\n    \"jackknife\": Params(method=\"base\", cv=-1),\n    \"jackknife_plus\": Params(method=\"plus\", cv=-1),\n    \"jackknife_minmax\": Params(method=\"minmax\", cv=-1),\n    \"cv\": Params(method=\"base\", cv=10),\n    \"cv_plus\": Params(method=\"plus\", cv=10),\n    \"cv_minmax\": Params(method=\"minmax\", cv=10),\n}\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(3*6, 12))\naxs = [ax1, ax2, ax3, ax4, ax5, ax6]\nfor i, (strategy, params) in enumerate(STRATEGIES.items()):\n    mapie = MapieRegressor(\n        polyn_model,\n        ensemble=True,\n        n_jobs=-1,\n        **params\n    )\n    mapie.fit(X_train.reshape(-1, 1), y_train)\n    y_pred, y_pis = mapie.predict(X_test.reshape(-1, 1), alpha=0.05,)\n    plot_1d_data(\n        X_train,\n        y_train,\n        X_test,\n        y_test,\n        y_test_sigma,\n        y_pred,\n        y_pis[:, 0, 0],\n        y_pis[:, 1, 0],\n        axs[i],\n        strategy\n    )\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}