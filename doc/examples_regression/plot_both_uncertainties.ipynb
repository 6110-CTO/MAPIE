{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Estimating aleatoric and epistemic uncertainties\nThis example uses :class:`mapie.regression.MapieRegressor` to estimate\nprediction intervals capturing both aleatoric and epistemic uncertainties\non a one-dimensional dataset with homoscedastic noise and normal sampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Any, TypeVar, Callable\nfrom typing_extensions import TypedDict\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom mapie.regression import MapieRegressor\nF = TypeVar(\"F\", bound=Callable[..., Any])\n\n\n# Functions for generating our dataset\ndef x_sinx(x: np.ndarray) -> Any:\n    \"\"\"One-dimensional x*sin(x) function.\"\"\"\n    return x*np.sin(x)\n\n\ndef get_1d_data_with_normal_distrib(\n    funct: F,\n    mu: float,\n    sigma: float,\n    n_samples: int,\n    noise: float\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate noisy 1D data with normal distribution from given function\n    and noise standard deviation.\n\n    Parameters\n    ----------\n    funct : F\n        Base function used to generate the dataset.\n    mu : float\n        Mean of normal training distribution.\n    sigma : float\n        Standard deviation of normal training distribution.\n    n_samples : int\n        Number of training samples.\n    noise : float\n        Standard deviation of noise.\n\n    Returns\n    -------\n    Tuple[Any, Any, np.ndarray, Any, float]\n        Generated training and test data.\n        [0]: X_train\n        [1]: y_train\n        [2]: X_test\n        [3]: y_test\n        [4]: y_mesh\n    \"\"\"\n    np.random.seed(42)\n    X_train = np.random.normal(mu, sigma, n_samples)\n    X_test = np.arange(mu - 4*sigma, mu + 4*sigma, sigma/20.)\n    y_train, y_mesh, y_test = funct(X_train), funct(X_test), funct(X_test)\n    y_train += np.random.normal(0, noise, y_train.shape[0])\n    y_test += np.random.normal(0, noise, y_test.shape[0])\n    return (\n        X_train.reshape(-1, 1), y_train, X_test.reshape(-1, 1), y_test, y_mesh\n    )\n\n\n# Data generation\nmu, sigma, n_samples, noise = 0, 2.5, 300, 0.5\nX_train, y_train, X_test, y_test, y_mesh = get_1d_data_with_normal_distrib(\n    x_sinx, mu, sigma, n_samples, noise\n)\n\n# Definition of our base model\ndegree_polyn = 10\npolyn_model = Pipeline(\n    [\n        (\"poly\", PolynomialFeatures(degree=degree_polyn)),\n        (\"linear\", LinearRegression())\n    ]\n)\n\n# Estimating prediction intervals\nParams = TypedDict(\"Params\", {\"method\": str, \"cv\": int})\nSTRATEGIES = {\n    \"jackknife_plus\": Params(method=\"plus\", cv=-1),\n    \"jackknife_minmax\": Params(method=\"minmax\", cv=-1),\n    \"cv_plus\": Params(method=\"plus\", cv=10),\n    \"cv_minmax\": Params(method=\"minmax\", cv=10),\n}\ny_pred, y_pis = {}, {}\nfor strategy, params in STRATEGIES.items():\n    mapie = MapieRegressor(polyn_model, ensemble=False, **params)\n    mapie.fit(X_train, y_train)\n    y_pred[strategy], y_pis[strategy] = mapie.predict(X_test, alpha=0.05)\n\n\n# Visualization\ndef plot_1d_data(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    y_test: np.ndarray,\n    y_sigma: float,\n    y_pred: np.ndarray,\n    y_pred_low: np.ndarray,\n    y_pred_up: np.ndarray,\n    ax: plt.Axes,\n    title: str\n) -> None:\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_xlim([-10, 10])\n    ax.set_ylim([np.min(y_test)*1.3, np.max(y_test)*1.3])\n    ax.fill_between(X_test, y_pred_low, y_pred_up, alpha=0.3)\n    ax.scatter(X_train, y_train, color=\"red\", alpha=0.3, label=\"Training data\")\n    ax.plot(X_test, y_test, color=\"gray\", label=\"True confidence intervals\")\n    ax.plot(X_test, y_test - y_sigma, color=\"gray\", ls=\"--\")\n    ax.plot(X_test, y_test + y_sigma, color=\"gray\", ls=\"--\")\n    ax.plot(X_test, y_pred, color=\"b\", alpha=0.5, label=\"Prediction intervals\")\n    if title is not None:\n        ax.set_title(title)\n    ax.legend()\n\n\nn_figs = len(STRATEGIES)\nfig, axs = plt.subplots(2, 2, figsize=(13, 12))\ncoords = [axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1]]\nfor strategy, coord in zip(STRATEGIES, coords):\n    plot_1d_data(\n        X_train.ravel(),\n        y_train.ravel(),\n        X_test.ravel(),\n        y_mesh.ravel(),\n        1.96*noise,\n        y_pred[strategy].ravel(),\n        y_pis[strategy][:, 0, 0].ravel(),\n        y_pis[strategy][:, 1, 0].ravel(),\n        ax=coord,\n        title=strategy\n    )\n\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\nax.set_xlim([-8, 8])\nax.set_ylim([0, 4])\nfor strategy in STRATEGIES:\n    ax.plot(X_test, y_pis[strategy][:, 1, 0] - y_pis[strategy][:, 0, 0])\nax.axhline(1.96*2*noise, ls=\"--\", color=\"k\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"Prediction Interval Width\")\nax.legend(list(STRATEGIES.keys()) + [\"True width\"], fontsize=8)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}