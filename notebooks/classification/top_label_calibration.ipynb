{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-label calibration for outputs of ML models\n",
    "\n",
    "The goal of this notebook is to present :class:`mapie.calibration.MapieCalibrator` by comparing it to the method presented in the paper for Top-label calibration [1].\n",
    "\n",
    "[1] Gupta, Chirag, and Aaditya K. Ramdas. \"Top-label calibration and multiclass-to-binary reductions.\" arXiv preprint arXiv:2107.08353 (2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AIgen/df-posthoc-calibration\n",
    "!cd df-posthoc-calibration && git checkout 109da93c1487cb38ee51fcac47088cdd29854999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scikit-learn-contrib/MAPIE/blob/master/notebooks/classification/top_label_calibration.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from mapie.calibration import MapieCalibrator\n",
    "from mapie.metrics import top_label_ece\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"df-posthoc-calibration/\")\n",
    "import assessment\n",
    "import calibration \n",
    "\n",
    "random_state = 20\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating a classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [(0, 3.5), (-2, 0), (2, 0)]\n",
    "covs = [np.eye(2), np.eye(2)*2, np.diag([5, 1])]\n",
    "x_min, x_max, y_min, y_max, step = -6, 8, -6, 8, 0.1\n",
    "n_samples = 1000\n",
    "n_classes = 3\n",
    "np.random.seed(42)\n",
    "X = np.vstack([\n",
    "    np.random.multivariate_normal(center, cov, n_samples)\n",
    "    for center, cov in zip(centers, covs)\n",
    "])\n",
    "y = np.hstack([np.full(n_samples, i) for i in range(n_classes)])\n",
    "y += 1\n",
    "\n",
    "X_train_cal, X_test, y_train_cal, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_state\n",
    ")\n",
    "X_train, X_cal, y_train, y_cal = train_test_split(\n",
    "    X_train_cal, y_train_cal, test_size=0.25, random_state=random_state\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting a classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=random_state)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the prediction probabilities using the trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_calib = clf.predict_proba(X_cal)\n",
    "preds_test = clf.predict_proba(X_test)\n",
    "arg_max_preds_test = clf.classes_[np.argmax(preds_test, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calibration of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using method from the paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_per_bin=50\n",
    "\n",
    "# initialize recalibrator and set number of points per bins\n",
    "hb = calibration.HB_toplabel(points_per_bin=points_per_bin)\n",
    "hb.fit(preds_calib, y_cal)\n",
    "\n",
    "# get histogram binning probabilities on test data\n",
    "preds_test_hb = hb.predict_proba(preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MAPIE `calibration.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapie_reg = MapieCalibrator(estimator=clf, cv=\"prefit\", calibrator=\"isotonic\")\n",
    "mapie_reg.fit(X_cal, y_cal)\n",
    "mapie_prob_preds = mapie_reg.predict_proba(X_test)\n",
    "mapie_preds = mapie_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification that the same predictions are made.\n",
    "np.testing.assert_array_equal(mapie_preds, clf.classes_[np.argmax(mapie_prob_preds, axis=1)])\n",
    "np.testing.assert_array_equal(mapie_preds, arg_max_preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluating the models using ECE and Reliability diagrams created in the paper.\n",
    "\n",
    "Note that since we use different calibration methods, the results are slightly different, however, we still find similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some plots\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5.5), constrained_layout=True)\n",
    "fig.suptitle('Top-label reliability diagrams', fontsize=30)\n",
    "\n",
    "assessment.toplabel_reliability_diagram(y_test, preds_test, ax=ax[0])\n",
    "ax[0].set_title(\"Base ResNet-50 model \\n (Top-label ECE = {:.3f})\".format(top_label_ece(y_test, np.max(preds_test, axis=1), arg_max_preds_test)))\n",
    "\n",
    "assessment.toplabel_reliability_diagram(y_test, preds_test_hb, arg_max_preds_test, ax=ax[1], color='g')\n",
    "ax[1].set_title(\"ResNet-50 + histogram binning calibration\\n (Top-label-ECE = {:.3f})\".format(top_label_ece(y_test, preds_test_hb, arg_max_preds_test)));\n",
    "\n",
    "assessment.toplabel_reliability_diagram(y_test, np.max(mapie_prob_preds, axis=1), arg_max_preds_test, ax=ax[2], color='g')\n",
    "ax[2].set_title(\"ResNet-50 + MAPIE calibration (using Platt)\\n (Top-label-ECE = {:.3f})\".format(top_label_ece(y_test, np.max(mapie_prob_preds, axis=1), arg_max_preds_test)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r -f df-posthoc-calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('mapie-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82e719626d3d099733a6ccd500fa89ccf91cff4cc1efa1563e3a615b3872c3ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
