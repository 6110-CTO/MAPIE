{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Nested cross-validation for estimating prediction intervals\n\nThis example compares non-nested and nested cross-validation strategies for\nestimating prediction intervals with :class:`mapie.regression.MapieRegressor`.\n\nIn the regular sequential method, a cross-validation parameter search is\ncarried out over the entire training set.\nThe model with the set of parameters that gives the best score is then used in\nMAPIE to estimate the prediction intervals associated with the predictions.\nA limitation of this method is that residuals used by MAPIE are computed on\nthe validation dataset, which can be subject to overfitting as far as\nhyperparameter tuning is concerned.\nThis fools MAPIE into being slightly too optimistic with confidence intervals.\n\nTo solve this problem, an alternative option is to perform a nested\ncross-validation parameter search directly within the MAPIE estimator on each\n*out-of-fold* dataset.\nFor each testing fold used by MAPIE to store residuals, an internal\ncross-validation occurs on the training fold, optimizing hyperparameters.\nThis ensures that residuals seen by MAPIE are never seen by the algorithm\nbeforehand. However, this method is much heavier computationally since\nit results in $N * P$ calculations, where *N* is the number of\n*out-of-fold* models and *P* the number of parameter search cross-validations,\nversus $N + P$ for the non-nested approach.\n\nHere, we compare the two strategies on the Boston dataset. We use the Random\nForest Regressor as a base regressor for the CV+ strategy. For the sake of\nlight computation, we adopt a RandomizedSearchCV parameter search strategy\nwith a low number of iterations and with a reproducible random state.\n\nThe two approaches give slightly different predictions with the nested CV\napproach estimating slightly larger prediction interval widths by a\nfew percents at most (apart from a handful of exceptions).\n\nFor this example, the two approaches result in identical scores and identical\neffective coverages.\n\nIn the general case, the recommended approach is to use nested\ncross-validation, since it does not underestimate residuals and hence\nprediction intervals. However, in this particular example, effective\ncoverages of both nested and non-nested methods are the same.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom scipy.stats import randint\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nfrom mapie.regression import MapieRegressor\nfrom mapie.metrics import regression_coverage_score\n\n# Load the Boston data\nX_boston, y_boston = load_boston(return_X_y=True)\n\n# Split the data into training and test sets.\nX_train, X_test, y_train, y_test = train_test_split(\n    X_boston, y_boston, test_size=0.2, random_state=42\n)\n\n# Define the Random Forest model as base regressor with parameter ranges.\nrf_model = RandomForestRegressor(random_state=59, verbose=0)\nrf_params = {\n    \"max_depth\": randint(2, 30),\n    \"n_estimators\": randint(10, 1e3)\n}\n\n# Cross-validation and prediction-interval parameters.\ncv = 5\nn_iter = 5\nalpha = 0.05\nrandom_state = 59\n\n# Non-nested approach with the CV+ strategy using the Random Forest model.\ncv_obj = RandomizedSearchCV(\n    rf_model,\n    param_distributions=rf_params,\n    n_iter=n_iter,\n    cv=cv,\n    scoring=\"neg_root_mean_squared_error\",\n    return_train_score=True,\n    verbose=0,\n    random_state=random_state,\n    n_jobs=-1,\n)\ncv_obj.fit(X_train, y_train)\nbest_est = cv_obj.best_estimator_\nmapie_non_nested = MapieRegressor(\n    best_est,\n    method=\"plus\",\n    cv=cv,\n    ensemble=True,\n    n_jobs=-1\n)\nmapie_non_nested.fit(X_train, y_train)\ny_pred_non_nested, y_pis_non_nested = mapie_non_nested.predict(\n    X_test, alpha=alpha\n)\nwidths_non_nested = y_pis_non_nested[:, 1, 0] - y_pis_non_nested[:, 0, 0]\ncoverage_non_nested = regression_coverage_score(\n    y_test, y_pis_non_nested[:, 0, 0], y_pis_non_nested[:, 1, 0]\n)\nscore_non_nested = mean_squared_error(\n    y_test, y_pred_non_nested, squared=False\n)\n\n# Nested approach with the CV+ strategy using the Random Forest model.\ncv_obj = RandomizedSearchCV(\n    rf_model,\n    param_distributions=rf_params,\n    n_iter=n_iter,\n    cv=cv,\n    scoring=\"neg_root_mean_squared_error\",\n    return_train_score=True,\n    verbose=0,\n    random_state=random_state,\n    n_jobs=-1,\n)\nmapie_nested = MapieRegressor(\n    cv_obj,\n    method=\"plus\",\n    cv=cv,\n    ensemble=True\n)\nmapie_nested.fit(X_train, y_train)\ny_pred_nested, y_pis_nested = mapie_nested.predict(X_test, alpha=alpha)\nwidths_nested = y_pis_nested[:, 1, 0] - y_pis_nested[:, 0, 0]\ncoverage_nested = regression_coverage_score(\n    y_test, y_pis_nested[:, 0, 0], y_pis_nested[:, 1, 0]\n)\nscore_nested = mean_squared_error(y_test, y_pred_nested, squared=False)\n\n# Print scores and effective coverages.\nprint(\n    \"Scores and effective coverages for the CV+ strategy using the \"\n    \"Random Forest model.\"\n)\nprint(\n    \"Score on the test set for the non-nested and nested CV approaches: \",\n    f\"{score_non_nested: .3f}, {score_nested: .3f}\"\n)\nprint(\n    \"Effective coverage on the test set for the non-nested \"\n    \"and nested CV approaches: \",\n    f\"{coverage_non_nested: .3f}, {coverage_nested: .3f}\"\n)\n\n# Compare prediction interval widths.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 6))\nmin_x = 14.5\nmax_x = 16.0\nax1.set_xlabel(\"Prediction interval width using the nested CV approach\")\nax1.set_ylabel(\"Prediction interval width using the non-nested CV approach\")\nax1.set_xlim([min_x, max_x])\nax1.set_ylim([min_x, max_x])\nax1.scatter(widths_nested, widths_non_nested)\nax1.plot([min_x, max_x], [min_x, max_x], ls=\"--\", color=\"k\")\nax2.axvline(x=0, color=\"r\", lw=2)\nax2.set_xlabel(\n    \"[width(non-nested CV) - width(nested CV)] / width(non-nested CV)\"\n)\nax2.set_ylabel(\"Counts\")\nax2.hist(\n    (widths_non_nested - widths_nested)/widths_non_nested,\n    bins=15,\n    edgecolor=\"black\"\n)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}